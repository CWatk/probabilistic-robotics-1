\documentclass{article}
\usepackage{amsmath}
\author{Erik Lee}
\date{2014-10-24}
\title{A digression on the mysteries of covariance matrices}
\begin{document}
\maketitle

I struggled for a long time trying to get some intuition about why it is that
the covariance matrix should be given by $\vec{v} \vec{v}^T$, where $\vec{v}$ is
the vector of coefficients for the source of variance in the equations of
motion. That sentence is a tortured mess, let me try it with math:

If we have these equations of motion:

\begin{gather}
  x_t = x_{t-1} + \dot{x}_{t-1} \Delta_t + \frac{\Delta_t^2}{2} \ddot{x}_t \\
  \dot{x}_t = \dot{x}_{t-1} + \Delta_t \ddot{x}_t
\end{gather}

And we want to figure out how variance of $\ddot{x}$ affects the variance of $x$
and $\dot{x}$, why should it be the case that we make a vector out of the
coefficients of $\ddot{x}$ for each equation, then multiply it by its transpose:

\begin{gather}
cov_a(x,\dot{x}) = \begin{pmatrix}\frac{\Delta_t^2}{2} \\ \Delta_t\end{pmatrix}
\begin{pmatrix}\frac{\Delta_t^2}{2} && \Delta_t\end{pmatrix} =
\begin{pmatrix}\frac{\Delta_t^4}{4} && \frac{\Delta_t^3}{2} \\
\frac{\Delta_t^3}{2} && t^2 \end{pmatrix}
\end{gather}

To try to figure this out, I decided to start at the base definition of
covariance and work from there. tl;dr: Since $x$ and $\dot{x}$ are linear
functions with respect to $\ddot{x}$, their covariance terms are just the
products of their coefficients. Their variances are the squares of their
coefficients, and if you do the number crunching you see that the vector product
above gives you all possible combinations of pairwise products between the
variables, so it's a convenient way to get the data you need. It also happens to
organize it in a way that's predictable, and can thus be used in the formula for
a multivariate gaussian distribution. Read on below if you want to see the math
I did to get there (it's not very hard, you can tell because I was able to do it
at 10:00 at night when I finally decided to just grind through it).


\end{document}
